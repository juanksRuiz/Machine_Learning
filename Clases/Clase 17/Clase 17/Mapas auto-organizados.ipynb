{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Self-Organizing Maps (redes de Kohonen)\n",
    "\n",
    "Alrededor de lso años 80, cuando estaba surgiendo el algoritmo de backpropagation para el entrenaiento de redes neuronales, surgio una nueva topología de red neuronal. Esta nueva topologia se inspira en la forma como el cerebro organiza zonas de procesamiento que se encuentran localizadas en una misma región, o regiones cernanas.\n",
    "\n",
    "## Arquitectura de las redes de Kohonen.\n",
    "\n",
    "La estructura de la red de kohonen se puede observar en la siguiente figura:\n",
    "\n",
    "\n",
    "<img src=\"img/kohonen.png\" width=\"400\">\n",
    "\n",
    "La red neuronal ubica sus neuronas en un latice que puede ser de dimensión 1 o 2. Este latice represneta la ubicación de las unidades neuronales. \n",
    "\n",
    "Aqui se tiene una capa de entrada, que consiste en los vectores $\\mathbf{x}_k\\in\\mathbb{R}^m$, donde $m$ representa el número de caracteristicas que contiene el vector, e $i$ representa la observación $i$, con $k=\\{1,2,\\ldots,N\\}$, con $N$ el número de observaciones. Este vector de entrada esta conectado a todas las unidades neuronales que conforman la capa de salida de la red (capa de Kohonen), por medio de un vector de pesos $\\boldsymbol\\omega_i$. DE for ma matricial, si suponemos que todas las observaciones se encuentran en una matrix $\\mathbf{X} \\in \\mathbb{R}^{N\\times m}$, entonces la matriz de pesos tendria las dimensiones $\\mathbf{W} \\in \\mathbb{R}^{m\\times n}$, con $n$ el numero de neuronas en la capa de Kohonen.\n",
    "\n",
    "De esta forma la salida total de la red estaria dada por:\n",
    "\n",
    "$$\\mathbf{Y}=\\mathbf{W}^{\\text{T}}\\mathbf{X}^{\\text{T}},$$\n",
    "\n",
    "es decir se esta haciendo un mapeo de datos que se encuentran en $\\mathbb{R}^N$, a datos en $\\mathbb{R}^n$, si $n<N$ se logra tener reducción de dimensionalidad.\n",
    "\n",
    "## Principio de Funcionamiento de un SOM\n",
    "\n",
    "La idea detras del funcionamiento de los mapas autoorganizados es poder utilizar aprendizaje no supervisado para tener una idea de como es la estructura de los datos que se usan en entrenamiento. \n",
    "\n",
    "Para lograr eso las redes de Khonen lo que hacen es aplicar un algoritmo competitivo para identificar que unidades neuronales son mas parecidas a la entrada. Luego de esto aplica un algoritmo colaborativo para identificar unidades neuronales cercanas, y finalmente, realiza una actualización de los pesos de tal forma que se actualizan los pesos de la unidad neuronal más cercana y sus vecinos. Esto se puede resumir en los siguientes pasos:\n",
    "\n",
    "1. **Competencia**: Se busca la unidad neuronal $i$ cuyos pesos $\\boldsymbol\\omega_i$ se parecen más a la entrada $\\mathbf{x}_j$, esto se puede lograr mediante la busqueda de la unidad neuronal con la distancia minima entre sus pesos y la entrada, o la unidad que maximiza el producto punto entre sus pesos y la entrada:\n",
    "\n",
    "$$\\min_{i}||\\mathbf{x}_j-\\boldsymbol\\omega_i||_2,$$\n",
    "\n",
    "o\n",
    "\n",
    "$$\\max_{i}\\mathbf{x}^T\\boldsymbol\\omega_i.$$\n",
    "\n",
    "\n",
    "2. **Colaboración:** Se identifican las neuronas cercanas a la neurona $i$, para esto primero utilizaremos una medida de distancia euclidea entre lso pesos de las unidades neuronales $d(i,k)=||\\boldsymbol\\omega_i-\\boldsymbol\\omega_k||_2$. Normalizamos esta distancia utilizando una **Radial basis function** para definir que tan cercana es una unidad neuronal a otra:\n",
    "\n",
    "$$h(i,j) = e^{\\left(-\\frac{d(i,j)}{2\\sigma^2}\\right)}.$$\n",
    "\n",
    "Esta unidad de medida es cercana a $1$ para neuronas cercanas y cercana a cero par aneuronas lejanas. $\\sigma^2$ es un parametro que controla la definición de cercano o lejano. Esta distancia s edenomina la distancia lateral entre neuronas.\n",
    "\n",
    "Es importante notar que al principio yo quiero que hayan muchos cambios en la red, para poder darle flexibilidad en su aprendizaje, pero a medida qu elos datos se van presentando, las regiones del mapa deberian especializarse. Para controlar esto se puede hacer qu eel valor de la desviación estandar de la función RBF cambie con el tiempo, es decir qu etenga l aforma:\n",
    "\n",
    "$$\\sigma(t)=\\sigma_0e^{-\\frac{t}{T}},$$\n",
    "\n",
    "donde $t$ me representa la epoca del entrenamiento,y $T$ es una constante. Esto garantiza qu eal principuio se tenga un vecindario grande, pero a medida que van pasando las epocas este vecindario se va haciendo más pequeño, lo que hace que ciertas regiones se especialicen en cierto conjunto de datos.\n",
    "\n",
    "3. **Aprendizaje (Actualización de pesos):** Inicialmente partimos de unos pesos aleatorios, y estos se actualizan de tal forma que para neuronas cuyos pesos sean similares al vector de entrada se fortalezca esa conección, y para unidades neuronales con pesos alejados del vector as epenalice la actualización. Esta formula esta dada por:\n",
    "\n",
    "$$\\boldsymbol\\omega_i(t+1)=\\boldsymbol\\omega_j(t)+\\Delta\\boldsymbol\\omega_j(t).$$\n",
    "\n",
    "Esta formula es muy parecida a la qu ese encontro para gradiente descendiente, y  en general para los modelos de aprendizaje qu ese han estudiado. Aqui es donde ocurre el aprendizaje. En este caso el valor de $\\Delta\\boldsymbol\\omega_j(t)$ esta dado por:\n",
    "\n",
    "$$\\Delta\\boldsymbol\\omega_j(t)=\\eta(t)h_{(i,j)}(t)(\\mathbf{x}-\\boldsymbol\\omega_j),$$\n",
    "\n",
    "donde $i$ representa la unidad neuronal más parecida a $\\mathbf{x}$, $h_{(i,j)}(t) = e^{\\left(-\\frac{d(i,j)}{2\\sigma(t)^2}\\right)}$, y $\\eta(t) = \\eta_0e^{-\\frac{t}{T_2}}$, con $T_2$ un aconstante, es decir la tasa de aprendizaje tmabién decrece con el tiempo. El decrecer la tasa de aprendizaje con el tiempo evita que la red neuronal tenga cambios abruptos una vez su estructura ha convergido.\n",
    "\n",
    "Normalmente se puede seleccionar como valores $\\eta_0=0.1$, $T_2=1000$, y $T = \\frac{1000}{log(\\sigma_0)}$, con $\\sigma_0$ muy grande al comienzo.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Consideraciones de las Redes de Kohonen\n",
    "\n",
    "1. **Convergencia:** Estas redes toman mucho tiempo en entrenar, el número de epocas de entrenamiento esta al rededor de los miles de epocas. Generalmente una regla empirica es utiizar algunos miles del número de unidades neuronales.\n",
    "\n",
    "\n",
    "2. **Training Sttopping:** Existen muchos criterios de parada que s epueden usar solos o una combinación de ellos. Entre estos criterios tenemos:\n",
    "    * No hay un cmabio notable en la actualización de los pesos. es decir que los $\\Delta\\boldsymbol\\omega_j$ son muy pequeños.\n",
    "    * No hay un cambio notable en el mapa de caracteristicas (feature map, Capa de kohonen)\n",
    "  \n",
    "  \n",
    "3. **Problemas:** Existen algunos porblemas inherentes con las redes de kohonen:\n",
    "    * Toma demasiado tiempo en converger.\n",
    "    * Puede tener diferentes resultados en la convergencia."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SOM in practice\n",
    "\n",
    "Los SOM se pueden utilizar en una gran variedad de aplciaciones. PAra entender un poco mejor el SOM veamoslos desde pun punto de vista de un algoritmo de clustering como k-means. Para este analisis estamos más interesados en los pesos de las unidades neuronales que en la salida de las mismas. De esta forma, podemos decir que los pesos de las unidades neuronales representan los centroides de $n$ clusters donde estan ubicados los datos.  Así, debido al critero de vecindad utilizado en su entrenamiento, las unidades neuronales cercanas entre si (centroides de clusters cercanos) se van a agrupar muy cerca la una de la otra, definiendo las caracteristicas del agrupamiento. En este sentido, los SOM me pueden particionar el espacio donde se encuentran los datos en regiones de elementos similares, sin determinar cuantos clusters deben haber.\n",
    "\n",
    "Por otro lado, se puede definir lo que se conoce como un **hit map**. Este mapa me representa un conteo que determina cuantas veces esa unidad neuronal fue seleccionada como la unidad más parecida a un dato de entrada (BMU-Best Matching Unit), varias observaciones de entrada pueden ser mapeados a una misma unidad neuronal, por lo tanto esto me genera por un lado una reducción en la cantidad de observaciones, esto es util en caso de querer realizar una reducción de numero de observaciones para entrenar un modelo. Por tor lado esto tambien me permite generar en el mapa de caracteristicas un aimagen de que regiones de la red estan siendo activadas para elementos que son comunes. Al agrupar estos elementos y asignarles un label se pueden identificar regiones de unidades neuronales que identifican un conjunto de elementos en particular. Esto se parece muho a la forma como el cerebro procesa la información.\n",
    "\n",
    "También se puede analizar  para cada cluster identificado la contribución de cada uno de los pesos (su magnitud) en la salida de la red. De esta forma se pueden identificar que caracteristicas (features) de lo datos de entrada son más relevantes para determinar los elementos qu epertenecen a un cluster. Esto sirve para realizar selección de caracteristicas.\n",
    "\n",
    "Se puede analizar como se distribuyen los clusters entre cada grupo analizando como es la distancia entre los pesos de una unidad neuronal y el peso de sus vecinos. De esta forma se puede lograr ver que tan parecidos son las unidades neuronales que ocnforman un cluster, y qu etan bien separados estan de los demas clusters. Esta representación me puede dar una idea en dos dimensiones de como estan organizados los datos en $N$ dimensiones. \n",
    "\n",
    "Finalmente, si tomamos en cuenta la salida de la red como un elemento en $\\mathbb{R}^{n}$, si mis datos de entrada estan mapeados en $\\mathbb{R}^{N}$, y si $n<N$ entonces se tiene que los SOM me permiten realizar una dreducción de dimensionalidad conservando las caracteristicas topologicas de mis datos de entrada, a diferencia de PCA. Esto significa qu elos SOM van a agrupar elementos que estan cercanos entre si en un espacio de dimesión $N$ a elementos cercamnos entre si en un espacio de dimensión menor, $n$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejemplos:\n",
    "\n",
    "### Fitting a distribution\n",
    "\n",
    "Las SOM pueden utilizarse para hacer un muestreo de la distribución de datos [aquí](https://www.youtube.com/watch?v=QvI6L-KqsT4) pueden ver un ejemplo. Otro [aquí](https://www.youtube.com/watch?v=k7DK5fnJH94).\n",
    "\n",
    "### Mapeo de Formas\n",
    "\n",
    "LAs SOM pueden utilizarse para realizar un mapeo de contornos. Un ejemlo se puede ver [aquí](https://www.youtube.com/watch?v=ipH_Df2MbPI). Otro ejemplo esta [aquí](https://www.youtube.com/watch?v=4FqkhiQcblc). Aplicado al Traveling Salesman Problem [aquí](https://www.youtube.com/watch?v=wT9rEi9dY8M). \n",
    "\n",
    "### Cuantización de Color\n",
    "\n",
    "Se pued utilizar también para hacer una mejor selección de paleta de colores para representar una imágen a color. Un ejemplo se puede ver [aquí](https://www.youtube.com/watch?v=ZCDF9f1Wo0Q).\n",
    "\n",
    "## Encontrar correlaciones entre los datos\n",
    "\n",
    "Se muestran los pesos correspondientes a cada parametro de entrada (feature) y esto produce una imagen que se puede identificar como un mapa de claor. De esta forma se puede determinar si hay features similares entre si. Estos se denominan component planes, un ejemplo s epuede ver [aquí](https://youtu.be/K4WuE7zlOZo?t=531).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
