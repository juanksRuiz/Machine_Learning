{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clase 5: Clasificación Lineal\n",
    "\n",
    "En esta clase estudiaremos los problemas de clasificación lineal, particularmenteLDA (Linear deiscriminant Analysis) y regresión Logística.\n",
    "\n",
    "**Pregunta:** ¿Porqué no es bueno utilizar un aregresión para solucionar un problema de clasificación?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solución:** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Discriminant Analysis (LDA), aproximación de Fisher\n",
    "\n",
    "Supongamos que tenemos un conjunto de datos $\\{(\\mathbf{x}^{(1)}, y^{(1)}), (\\mathbf{x}^{(2)}, y^{(2)}), \\ldots, (\\mathbf{x}^{(m)}, y^{(m)})\\}$, donde $\\mathbf{x}^{(i)}\\in\\mathbb{R}^n$ y $y \\in\\{0,1\\}$. Sea $\\text{p}(\\mathbf{x}|y=0)$ y $\\text{p}(\\mathbf{x}|y=1)$ las distribuciones de probabilidad para los valroes de $\\mathbf{x}$ que perteneces a la calse $y = 0$ y $y=1$, respectivamente. La idea detras del LDA es tratar de encontrar la dirección de un eje, de tal forma que al proyectar $\\mathbf{x}$ sobre este se maximiza la separación entre las medias de las distribuciones de probabilidad de las variables proyectadas, y al mismo tiempo se minimizan sus varianzas. En otras palabras lo que busca LDA es realizar una transformación lineal de los datos de entrada, tal que se maximice su disperción entre clases, y se minimice su disperción dentro de cada clase. La siguiente figura explica este proceso de forma grafica:\n",
    "![title](img/LDA.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ¿Cómo se plantea este problema?\n",
    "\n",
    "Para solucionar este problema debemos plantear la función de costo.\n",
    "\n",
    "**Pregunta:** ¿Cuál sería esa función de costo?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solución:** Esa función de costo se define de la siguiente forma:\n",
    "\n",
    "sea $\\hat{y} = \\theta_0+\\boldsymbol\\theta^T\\mathbf{x}$ la salida estimada de mi modelo de clasificación, teniendo como base esto, el valor medio de esta salida está dado por:\n",
    "\n",
    "$$\\begin{equation}\n",
    "\\boldsymbol\\mu_i=\\frac{1}{n_i}\\sum_{k\\in C_i}\\mathbf{x}^{(k)}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "donde $C_i$ es el conjunto de datos de entrada $\\mathbf{x}$ que pertenecen a la clase $i$, $\\mu_i$ es la media de estos elementos, y $n_i$ es el número de muestras que pertencen a esa clase. y la varianza está dada por:\n",
    "\n",
    "$$\\begin{equation}\n",
    "\\boldsymbol\\Sigma_i=\\frac{1}{(n_i-1)}\\sum_{k\\in C_i}(\\mathbf{x}^{(k)}-\\boldsymbol\\mu_k)(\\mathbf{x}^{(k)}-\\boldsymbol\\mu_k)^\\text{T}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "donde $\\text{T}$ es la transpuesta.\n",
    "\n",
    "De aquí se puede definir la media y la varianza de los valores predecidos de $y$, de forma que:\n",
    "\n",
    "sea $\\hat{y}= \\theta_0+\\boldsymbol\\theta^{\\text{T}}\\mathbf{x}$, se tiene:\n",
    "\n",
    "$$\\begin{equation}\n",
    "\\begin{split}\n",
    "\\text{E}[\\hat{y}|\\mathbf{x}\\in C_i] & = & \\theta_0 +\\boldsymbol\\theta^{\\text{T}}\\boldsymbol\\mu_i \\\\\n",
    "\\text{Var}[\\hat{y}|\\mathbf{x}\\in C_i] &=& \\boldsymbol\\theta^{\\text{T}}\\boldsymbol\\Sigma_i\\boldsymbol\\theta\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "De allí la función de costo, asuminedo dos clases, es:\n",
    "\n",
    "$$\\boldsymbol\\theta := \\max\\limits_\\boldsymbol\\theta \\frac{(\\boldsymbol\\mu_0^{\\text{T}}\\boldsymbol\\theta-\\boldsymbol\\mu_1^{\\text{T}}\\boldsymbol\\theta)^2}{\\left(n_0\\boldsymbol\\theta^{\\text{T}}\\boldsymbol\\Sigma_0\\boldsymbol\\theta+n_1\\boldsymbol\\theta^{\\text{T}}\\boldsymbol\\Sigma_1\\boldsymbol\\theta\\right)} = \\frac{\\left((\\boldsymbol\\mu_0^{\\text{T}}-\\boldsymbol\\mu_1^{\\text{T}})\\boldsymbol\\theta\\right)^2}{\\boldsymbol\\theta^{\\text{T}}\\left(n_0\\boldsymbol\\Sigma_0+n_1\\boldsymbol\\Sigma_1\\right)\\boldsymbol\\theta}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pregunta:** ¿Cómo maximizar esa función?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solución:** \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "se busca maximizar ese producto punto, el cual es máximo si los dos vectores estan en la misma dirección. Propongamos que $\\mathbf{v}= a (\\mathbf{R}^{-1})^\\text{T}\\mathbf{m}$, donde $a$ es una constante, es el vector qu emáximiza ee producto punto. Tomando de antes $\\mathbf{m} = \\boldsymbol\\mu_0-\\boldsymbol\\mu_1$, y $\\boldsymbol\\theta=\\mathbf{R}^{-1}\\mathbf{v}$ se tiene:\n",
    "\n",
    "$$ \\mathbf{v} = a\\mathbf{R}^{-1}(\\mathbf{R^{-1}})^{\\text{T}}(\\boldsymbol\\mu_0-\\boldsymbol\\mu_1)=a(\\mathbf{R}^\\text{T}\\mathbf{R})^{-1}(\\boldsymbol\\mu_0-\\boldsymbol\\mu_1),$$\n",
    "\n",
    "lo cual se simplifica por:\n",
    "\n",
    "$$\\boldsymbol\\theta = a\\mathbf{S}^{-1}(\\boldsymbol\\mu_0-\\boldsymbol\\mu_1)=a\\left(n_0\\boldsymbol\\Sigma_0+n_1\\boldsymbol\\Sigma_1\\right)^{-1}(\\boldsymbol\\mu_0-\\boldsymbol\\mu_1),$$\n",
    "\n",
    "con $a$ una constante arbitraria que puede ser 1. De allí:\n",
    "$$\\theta_0 = \\text{E}[y-\\boldsymbol\\theta^\\text{T}\\mathbf{x}] = \\frac{1}{n}\\sum y^{(i)}-\\boldsymbol\\theta^\\text{T}\\mathbf{x}^{(i)}.$$\n",
    "\n",
    "De forma sencilla, lo que se esta haciendo es tomar el vector de la diferencia de ambas medias, y se rota por medio de la inversa de las covarianzas de los datos, i.e. donde mas varian los datos tiene el menor efecto en esta rotación."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Nota:** LDA considera que los datos en cada clase estan distribuidos de forma Normal, y que las matrices de covarianza de las distriuciones son iguales. Fisher (tal como lo vimos aquí) no requiere de easumir estás cosas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LDA a traves de Bayes\n",
    "\n",
    "Se puede llegar a LDA a traves de Bayes, sabiendo que:\n",
    "\n",
    "$$\\text{p}(y=1|\\mathbf{x})=\\frac{\\text{p}(\\mathbf{x}|y=1)\\text{p}(y=1)}{\\text{p}(\\mathbf{x})},$$\n",
    "\n",
    "donde $\\text{p}(\\mathbf{x}) = \\text{p}(\\mathbf{x}|y=1)\\text{p}(y=1)+\\text{p}(\\mathbf{x}|y=0)\\text{p}(y=0)$. En términos de probabilidades el likelihood (similaridad) es $\\text{p}(\\mathbf{x}|y=1)$, el prior es $\\text{p}(y=1)$ y la probabilidad marginal es $\\text{p}(\\mathbf{x})$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regresión Logística\n",
    "\n",
    "La regresión logística es un metodo de clasificación, a pesar de que se conoce como regresión. Para la clasificación quisieramos que lso valores de salida de nuestro modelo este acotado entre cero (o -1) y uno. Par alograr esto, asunmamos nuestro modelo tiene la forma:\n",
    "\n",
    "$$h_{\\boldsymbol\\theta}(\\mathbf{x}) = g(\\boldsymbol\\theta^{\\text{T}}\\mathbf{x}),$$\n",
    "\n",
    "donde $g(\\displaystyle\\mathbf{z})=\\frac{1}{1+e^{-\\mathbf{z}}}$ es una función sigmoidal o logistica. Por lo tanto la forma del modelo es:\n",
    "\n",
    "$$h_{\\boldsymbol\\theta}(\\mathbf{x}) = \\frac{1}{1+e^{-\\boldsymbol\\theta^{\\text{T}}\\mathbf{x}}}.$$\n",
    "\n",
    "La función logistica luce como se muestra en la siguiente figura:\n",
    "\n",
    "<img src=\"img/logit.png\" width=\"400\">\n",
    "\n",
    "El problema se reduce a encontrar lso valores de $\\boldsymbol\\theta$ que garantizan que tenga una buena clasificación. \n",
    "\n",
    "En el caso de un modelo de clasificación para dos clases, la salida de este modelo se puede interpretar como la probabilidad de que la salida sea $y=1$ cuando la entrada es $\\mathbf{x}$, i.e. $\\text{p}(y=1|\\mathbf{x};\\boldsymbol\\theta)$. Teniendo en cuenta esto, y debido a que las probabilidades de pertenecer a una clase o la otra deben sumar uno, se tiene que $\\text{p}(y=0|\\mathbf{x};\\boldsymbol\\theta) = 1-\\text{p}(y=1|\\mathbf{x};\\boldsymbol\\theta)$.\n",
    "\n",
    "Para terminar se puede entonces definir la salida de tal forma que:\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "  \\hat{y} =\n",
    "    \\begin{cases}\n",
    "      1 & \\text{if} & h_{\\boldsymbol\\theta}(\\mathbf{x}) \\geq 0.5\\\\\n",
    "      0 & \\text{if} & h_{\\boldsymbol\\theta}(\\mathbf{x}) < 0.5\n",
    "    \\end{cases}       \n",
    "\\end{equation}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Limite de Desición"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pregunta:** ¿Cuando la función logistica será mayor a 0.5?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solución:** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pregunta:** ¿Qué tipo de limite de desición forma el modelo $h_{\\boldsymbol\\theta}(\\mathbf{x}) = g([-1;1;1]^\\text{T}[1;x_1^2;x_2^2])$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solución:** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Nota:** EL limite de desición depende del modelo, no de los datos, los datos se utilizan solo para calcular los parametros."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ajuste del modelo\n",
    "\n",
    "Sea nuestro vector $\\mathbf{x}^{(i)} = [x_0^{(i)};x_1^{(i)};\\ldots;x_n^{(i)}$, con $x_0^{(i)} = 1$, para todo $i$, y $\\mathbf{x}^{(i)} \\in \\mathbb{R}^{n+1}$ y $y\\in\\{0,1\\}$. Dado el set de entrenamiento como se encuentran los parámetros $\\boldsymbol\\theta$.\n",
    "\n",
    "Definamos la función de costo para nuestra regresión:\n",
    "\n",
    "$$\\mathbf{J}(\\boldsymbol\\theta) = \\frac{1}{m}\\sum_{i=1}^{m}\\text{costo}(h_{\\boldsymbol\\theta}(\\mathbf{x}^{(i)}),y^{(i)}),$$\n",
    "\n",
    "donde $$\\text{costo}(h_{\\boldsymbol\\theta}(\\mathbf{x}^{(i)}),y^{(i)})= \\frac{1}{2}\\left(h_{\\boldsymbol\\theta}(\\mathbf{x}^{(i)})-y^{(i)}\\right)^2.$$\n",
    "\n",
    "**Pregunta:** ¿Cuál es el problema de esta función de costo?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solución:** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La nueva función de costo para este problema es:\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "  \\text{costo}(h_\\boldsymbol\\theta(\\mathbf{x}),y) =\n",
    "    \\begin{cases}\n",
    "      -\\log(h_\\boldsymbol\\theta(\\mathbf{x})) & \\text{if} & y=1\\\\\n",
    "      -log(1-h_\\boldsymbol\\theta(\\mathbf{x})) & \\text{if} & y=0\n",
    "    \\end{cases}       \n",
    "\\end{equation}$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pregunta:** ¿Cómo luce la función de costo de la regresión logistica?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solución:** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esta función de costo garantiza que si mi predicción es correcta $h_\\boldsymbol\\theta(\\mathbf{x}) = y$ entonces el costo es 0, si la predicción es incorrecta, el costo tiende al $\\infty$. Además esta función es convexa, lo cual facilita qla aplicación de gradient descent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Teniendo en cuenta este nuevo costo, se obtiene que:\n",
    "\n",
    " $$ \\text{costo}(h_\\boldsymbol\\theta(\\mathbf{x}),y) = -y\\log(h_\\boldsymbol\\theta(\\mathbf{x}))-(1-y)\\log(1-h_\\boldsymbol\\theta(\\mathbf{x})).$$\n",
    " \n",
    " De allí la función de costo para la regresión logística estará dada por:\n",
    " \n",
    " $$\\mathbf{J}(\\boldsymbol\\theta) = -\\frac{1}{m}\\sum_{i=1}^{m}y^{(i)}\\log(h_\\boldsymbol\\theta(\\mathbf{x}^{(i)}))-(1-y^{(i)})\\log(1-h_\\boldsymbol\\theta(\\mathbf{x}^{(i)})).$$\n",
    " \n",
    "Lo único que queda por hacer es encontrar los parámetros $\\boldsymbol\\theta$ que minimizan esa función de costo. Para hacer esto se utiliza gradient descent.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pregunta:** ¿Qué necesitamos para poder desarrollar el gradient desecent?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solución:** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pregunta:** ¿Cuanto da la derivada de la función de costo en función de los parámetros?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solución:** La derivada tiene como resultado:\n",
    "\n",
    "$$\\frac{\\partial\\mathbf{J}(\\boldsymbol\\theta)}{\\partial\\theta_j} = \\frac{1}{m}\\sum_{i=1}^m\\left(h_\\boldsymbol\\theta(\\mathbf{x}^{(i)})-y^{(i)}\\right)x_j^{(i)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Así, la regla de actualización de los pesos en gradient descent, par ala regresión logistica, esta dada por:\n",
    "\n",
    "$$ \\begin{equation}\n",
    "      \\theta_n := \\theta_n-\\alpha\\frac{1}{m}\\sum_{i=1}^{m}\\left[h_\\boldsymbol\\theta(\\mathbf{x}^{(i)})-y^{(i)}\\right]x_n^{(i)}\\\\\n",
    "   \\end{equation}$$\n",
    "   \n",
    "Donde $x_0^{(i)}= 1$ para todo $i$. aunque luce identico al gradiente descendiente para regresión lineal, no lo es, ya que la función $h_\\boldsymbol\\theta(\\mathbf{x})$ es diferente a la función de regresión lineal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Nota 1:** La ormalización de caracteristicas (regresores) también aplica para gradeinte descendiente cuando se usa la regresión logistica. De la misma forma que aplcia en regresion lineal.\n",
    "\n",
    "**Nota 2:** Aparte de gradeint descent existen otros algoritmos más sofisticados qu epermiten encontrar los parámetros que minimizan la función de costo. Generalmente ellos requieren una función para el cálculo del costo y de su derivada en función de los parámetros. además, ellos son más rápidos y no requieren de sintonizar la taza de aprendizaje, pero son más complicados. Estos algoritmos no se estudiaran en el curso, debieron verse en Optimización, (Conjugate gradient, BFGS, L-BGFS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regresión Logistica Multiclase\n",
    "\n",
    "Si se tienen $p$ clases, se pueden entrenar $p$ clasificadores, utilizando regresión logistica, y al final se agregan las salidas de cada clasificador, de tal forma que la clase seleccionada se aaquella que tenga la mayor probabilidad (la mayor variable latente de salida del clasificador). Tal como se explica en la figura:\n",
    "\n",
    "<img src=\"img/onevsrest.png\" width=\"400\">\n",
    "\n",
    "**Pregunta:** ¿Qu'e sucede si no escojo al final la clase que de la mayor probabilidad, sino que escojo en función de si la probabilidad es mayor a 0.5?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solución:** si se realiza de esta forma la asignación de la clase se puede crear una región de ambiguedad, tal como se muestra en la figura de abajo:\n",
    "\n",
    "<img src=\"img/Ambiguedad.png\" width=\"200\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "matematicamente, la salida del clasificador está dada por: \n",
    "\n",
    "$$\\hat{y} = \\max\\limits_ih^{(i)}_\\boldsymbol\\theta(\\mathbf{x}),$$\n",
    "\n",
    "donde $h^{(i)}_\\boldsymbol\\theta(\\mathbf{x})$ es la salida del clasificador entrenado para la clase $i$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularización en Regresión Logistica\n",
    "\n",
    "La regresión logistica puede tambien llegar a ser sobre-ajustada.\n",
    "\n",
    "**Pregunta:** ¿Qué significa que la regresión logistica pueda ser sobre-ajustada?¿Cómo luce un clasificador sobreajustado?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solución:** Un clasificador sobreajustado genera climites de desición extremadamente complicados, no suaves, que no son capaces de generalizar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para evitar el sobreajuste en la regresión logistica, podemos usar el mismo criterio que en regresión, disminuir el número de  regresores, o utilizar regularización, tipo LASSO, Tickhonov, etc... De está forma la nueva función de costo será:\n",
    "\n",
    " $$\\mathbf{J}(\\boldsymbol\\theta) = -\\frac{1}{m}\\sum_{i=1}^{m}y^{(i)}\\log(h_\\boldsymbol\\theta(\\mathbf{x}^{(i)}))-(1-y^{(i)})\\log(1-h_\\boldsymbol\\theta(\\mathbf{x}^{(i)}))+\\frac{\\gamma}{2m}\\sum_{j=1}^n\\theta_j^2.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La actualización en gradiente descendiente sería:\n",
    "\n",
    "$$ \\begin{equation}\n",
    "\\begin{split}\n",
    "    \\theta_0&:=& \\theta_0-\\alpha\\frac{1}{m}\\sum_{i=1}^{m}\\left[h_\\boldsymbol\\theta(\\mathbf{x}^{(i)})-y^{(i)}\\right]x_0^{(i)}\\\\\n",
    "\\theta_j &:=& \\theta_n-\\alpha\\left[\\frac{1}{m}\\sum_{i=1}^{m}\\left[h_\\boldsymbol\\theta(\\mathbf{x}^{(i)})-y^{(i)}\\right]x_j^{(i)}+\\frac{\\gamma}{m}\\theta_j\\right]\\\\\n",
    "\\end{split}\n",
    "\\end{equation}$$\n",
    "\n",
    "con $j={1,\\ldots,n}$. Donde el modelo $h_\\boldsymbol\\theta(\\mathbf{x})$ es la función logistica."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
